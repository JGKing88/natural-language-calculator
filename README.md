# natural-language-calculator
To practice manipulating a transformer I'm using a transformer to build a calculator that processes natural language. This transformer is just the decoder half of the tranformer from the original Attention Is All You Need paper, except the normalization layer now comes before the attention and ffw blocks as is now standard practice. 
